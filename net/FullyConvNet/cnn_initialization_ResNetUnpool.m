function net = cnn_initialization_ResNetUnpool(varargin)

opts.scale = 1;   %for individual learning rates
opts.weightDecay = [1,0];

%% Load ResNet and remove last layers
net = load('ResNet\imagenet-resnet-50-dag.mat');
net = dagnn.DagNN.loadobj(net) ;
net.removeLayer('pool5');   %last pooling
net.removeLayer('prob');    %softmax
net.removeLayer('fc1000');  %fully-connected layer
n_previous_layers = numel(net.layers);

% % Fix BatchNorm kernel issues (already fixed - this was for an older version)
% for i=1:numel(net.layers)
%     if isa(net.layers(i).block,'dagnn.BatchNorm')
%         p = net.getParamIndex(net.layers(i).params);
%         net.params(p(1)).value = reshape(net.params(p(1)).value, [], 1);
%         net.params(p(2)).value = reshape(net.params(p(2)).value, [], 1);
%     end
% end


%% -------------------------------------------------------------------------
%% Repurpose net as FCN
%% -------------------------------------------------------------------------

%% ---------- Transit layer -----------------------------------------------
%(just reducing the number of channels)
 net.addLayer('layer1', ...
    dagnn.Conv('size', [1,1,2048,1024], 'pad', 0, 'stride', 1), ...
    {'res5cx'}, ...
    {'x1'}, ...
    {'layer1_f','layer1_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('layer1_BN', ...
    dagnn.BatchNorm('numChannels', 1024), ...
    {'x1'}, ...
    {'x2'}, ...
    {'layer1BN_mult', 'layer1BN_bias', 'layer1BN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

%% ------Up-projection #1--------------------------------------------------

net.addLayer('unpool1', ... %2x
    dagnn.Unpooling(), ...
    {'x2'}, ...
    {'x3'});

% --- Branch 1 (unpool1) --------------------------------------------------
net.addLayer('UpConv1a', ...
    dagnn.Conv('size', [5,5,1024,512], 'pad', 2, 'stride', 1), ...
    {'x3'}, ...
    {'x4'}, ...
    {'UpConv1a_f','UpConv1a_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv1a_BN', ...
    dagnn.BatchNorm('numChannels', 512), ...
    {'x4'}, ...
    {'x5'}, ...
    {'UpConv1aBN_mult', 'UpConv1aBN_bias', 'UpConv1aBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpConv1a_ReLU', ...
    dagnn.ReLU(), ...
    {'x5'}, ...
    {'x6'});

net.addLayer('UpConv1aa', ...
    dagnn.Conv('size', [3,3,512,512], 'pad', 1, 'stride', 1), ...
    {'x6'}, ...
    {'x7'}, ...
    {'UpConv1aa_f','UpConv1aa_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv1aa_BN', ...
    dagnn.BatchNorm('numChannels', 512), ...
    {'x7'}, ...
    {'x8'}, ...
    {'UpConv1aaBN_mult', 'UpConv1aaBN_bias', 'UpConv1aaBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

% --- Branch 2 (unpool1) --------------------------------------------------
net.addLayer('UpConv1b', ...
    dagnn.Conv('size', [5,5,1024,512], 'pad', 2, 'stride', 1), ...
    {'x3'}, ...
    {'x4b'}, ...
    {'UpConv1b_f','UpConv1b_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv1b_BN', ...
    dagnn.BatchNorm('numChannels', 512), ...
    {'x4b'}, ...
    {'x5b'}, ...
    {'UpConv1bBN_mult', 'UpConv1bBN_bias', 'UpConv1bBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpSum1', ...
    dagnn.Sum(), ...
    {'x8', 'x5b'}, ...
    {'UpSum1_out'});

net.addLayer('UpSum1_ReLU', ...
    dagnn.ReLU(), ...
    {'UpSum1_out'}, ...
    {'UpSum1_relu'});

%% ------Up-projection #2--------------------------------------------------

net.addLayer('unpool2', ... %4x
    dagnn.Unpooling(), ...
    {'UpSum1_relu'}, ...
    {'x1_2'});

% ---- Branch 1 (unpool2) -------------------------------------------------
net.addLayer('UpConv2a', ...
    dagnn.Conv('size', [5,5,512,256], 'pad', 2, 'stride', 1), ...
    {'x1_2'}, ...
    {'x2_2'}, ...
    {'UpConv2a_f','UpConv2a_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv2a_BN', ...
    dagnn.BatchNorm('numChannels', 256), ...
    {'x2_2'}, ...
    {'x3_2'}, ...
    {'UpConv2aBN_mult', 'UpConv2aBN_bias', 'UpConv2aBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpConv2a_ReLU', ...
    dagnn.ReLU(), ...
    {'x3_2'}, ...
    {'x4_2'});

net.addLayer('UpConv2aa', ...
    dagnn.Conv('size', [3,3,256,256], 'pad', 1, 'stride', 1), ...
    {'x4_2'}, ...
    {'x5_2'}, ...
    {'UpConv2aa_f','UpConv2aa_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv2aa_BN', ...
    dagnn.BatchNorm('numChannels', 256), ...
    {'x5_2'}, ...
    {'x6_2'}, ...
    {'UpConv2aaBN_mult', 'UpConv2aaBN_bias', 'UpConv2aaBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

% --- Branch 2 (unpool2) --------------------------------------------------
net.addLayer('UpConv2b', ...
    dagnn.Conv('size', [5,5,512,256], 'pad', 2, 'stride', 1), ...
    {'x1_2'}, ...
    {'x2b_2'}, ...
    {'UpConv2b_f','UpConv2b_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv2b_BN', ...
    dagnn.BatchNorm('numChannels', 256), ...
    {'x2b_2'}, ...
    {'x3b_2'}, ...
    {'UpConv2bBN_mult', 'UpConv2bBN_bias', 'UpConv2bBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpSum2', ...
    dagnn.Sum(), ...
    {'x6_2', 'x3b_2'}, ...
    {'UpSum2_out'});

net.addLayer('UpSum2_ReLU', ...
    dagnn.ReLU(), ...
    {'UpSum2_out'}, ...
    {'UpSum2_relu'});

%% ------Up-projection #3--------------------------------------------------

net.addLayer('unpool3', ... %8x
    dagnn.Unpooling(), ...
    {'UpSum2_relu'}, ...
    {'x1_3'});

% --- Branch 1 (unpool3) --------------------------------------------------
net.addLayer('UpConv3a', ...
    dagnn.Conv('size', [5,5,256,128], 'pad', 2, 'stride', 1), ...
    {'x1_3'}, ...
    {'x2_3'}, ...
    {'UpConv3a_f','UpConv3a_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv3a_BN', ...
    dagnn.BatchNorm('numChannels', 128), ...
    {'x2_3'}, ...
    {'x3_3'}, ...
    {'UpConv3aBN_mult', 'UpConv3aBN_bias', 'UpConv3aBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpConv3a_ReLU', ...
    dagnn.ReLU(), ...
    {'x3_3'}, ...
    {'x4_3'});

net.addLayer('UpConv3aa', ...
    dagnn.Conv('size', [3,3,128,128], 'pad', 1, 'stride', 1), ...
    {'x4_3'}, ...
    {'x5_3'}, ...
    {'UpConv3aa_f','UpConv3aa_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv3aa_BN', ...
    dagnn.BatchNorm('numChannels', 128), ...
    {'x5_3'}, ...
    {'x6_3'}, ...
    {'UpConv3aaBN_mult', 'UpConv3aaBN_bias', 'UpConv3aaBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

% --- Branch 2 (unpool3) --------------------------------------------------
net.addLayer('UpConv3b', ...
    dagnn.Conv('size', [5,5,256,128], 'pad', 2, 'stride', 1), ...
    {'x1_3'}, ...
    {'x2b_3'}, ...
    {'UpConv3b_f','UpConv3b_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv3b_BN', ...
    dagnn.BatchNorm('numChannels', 128), ...
    {'x2b_3'}, ...
    {'x3b_3'}, ...
    {'UpConv3bBN_mult', 'UpConv3bBN_bias', 'UpConv3bBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpSum3', ...
    dagnn.Sum(), ...
    {'x6_3', 'x3b_3'}, ...
    {'UpSum3_out'});

net.addLayer('UpSum3_ReLU', ...
    dagnn.ReLU(), ...
    {'UpSum3_out'}, ...
    {'UpSum3_relu'});

%% ------Up-projection #4--------------------------------------------------

net.addLayer('unpool4', ... %16x
    dagnn.Unpooling(), ...
    {'UpSum3_relu'}, ...
    {'x1_4'});

% --- Branch 1 (unpool4) --------------------------------------------------
net.addLayer('UpConv4a', ...
    dagnn.Conv('size', [5,5,128,64], 'pad', 2, 'stride', 1), ...
    {'x1_4'}, ...
    {'x2_4'}, ...
    {'UpConv4a_f','UpConv4a_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv4a_BN', ...
    dagnn.BatchNorm('numChannels', 64), ...
    {'x2_4'}, ...
    {'x3_4'}, ...
    {'UpConv4aBN_mult', 'UpConv4aBN_bias', 'UpConv4aBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpConv4a_ReLU', ...
    dagnn.ReLU(), ...
    {'x3_4'}, ...
    {'x4_4'});

net.addLayer('UpConv4aa', ...
    dagnn.Conv('size', [3,3,64,64], 'pad', 1, 'stride', 1), ...
    {'x4_4'}, ...
    {'x5_4'}, ...
    {'UpConv4aa_f','UpConv4aa_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv4aa_BN', ...
    dagnn.BatchNorm('numChannels', 64), ...
    {'x5_4'}, ...
    {'x6_4'}, ...
    {'UpConv4aaBN_mult', 'UpConv4aaBN_bias', 'UpConv4aaBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

% --- Branch 2 (unpool4) --------------------------------------------------
net.addLayer('UpConv4b', ...
    dagnn.Conv('size', [5,5,128,64], 'pad', 2, 'stride', 1), ...
    {'x1_4'}, ...
    {'x2b_4'}, ...
    {'UpConv4b_f','UpConv4b_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpnConv4b_BN', ...
    dagnn.BatchNorm('numChannels', 64), ...
    {'x2b_4'}, ...
    {'x3b_4'}, ...
    {'UpConv4bBN_mult', 'UpConv4bBN_bias', 'UpConv4bBN_moments'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;

net.addLayer('UpSum4', ...
    dagnn.Sum(), ...
    {'x6_4', 'x3b_4'}, ...
    {'UpSum4_out'});

net.addLayer('UpSum4_ReLU', ...
    dagnn.ReLU(), ...
    {'UpSum4_out'}, ...
    {'UpSum4_relu'});

net.addLayer('drop', ...
    dagnn.DropOut(), ...
    {'UpSum4_relu'}, ...
    {'dropped'});

%% Prediction 

net.addLayer('ConvPred', ...
    dagnn.Conv('size', [3,3,64,2], 'pad', 1, 'stride', 1), ...
    {'UpSum4_relu'}, ...
    {'prediction'}, ...
    {'ConvPred_f','ConvPred_b'});
params = net.params(net.layers(numel(net.layers)).paramIndexes);
[params.learningRate] = deal(opts.scale, opts.scale);
[params.weightDecay] = deal(opts.weightDecay(1), opts.weightDecay(1));
net.params(net.layers(numel(net.layers)).paramIndexes) = params;


%% Initialize parameters for new layers

for l = n_previous_layers+1:numel(net.layers)
  p = net.getParamIndex(net.layers(l).params) ;
  params = net.layers(l).block.initParams() ;
  switch net.device
    case 'cpu'
      params = cellfun(@gather, params, 'UniformOutput', false) ;
    case 'gpu'
      params = cellfun(@gpuArray, params, 'UniformOutput', false) ;
  end
  [net.params(p).value] = deal(params{:}) ;
end

%% ---------- Loss - Errors -----------------------------------------------

net.addLayer('loss', dagnn.SegmentationLoss('loss', 'softmaxlog'),{'prediction', 'label'}, 'objective') ;

net.addLayer('accuracy',dagnn.SegmentationAccuracy(), {'prediction', 'label'}, 'accuracy') ;
      

    
end

